"""
Modal Cloud ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ - Wav2Lipçµ±åˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
Gradio UIã‚’å«ã‚€å®Œå…¨çµ±åˆç‰ˆ
"""
import modal
from pathlib import Path

app = modal.App("wav2lip-integrated")

# ãƒœãƒªãƒ¥ãƒ¼ãƒ å®šç¾©
models_volume = modal.Volume.from_name("wav2lip-models-2025", create_if_missing=True)
output_volume = modal.Volume.from_name("wav2lip-outputs", create_if_missing=True)

# Dockerã‚¤ãƒ¡ãƒ¼ã‚¸å®šç¾©
image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install([
        "git", "ffmpeg", "libgl1-mesa-glx", "libglib2.0-0", "libsm6",
        "libxext6", "libxrender1", "libfontconfig1", "libice6",
        "libgomp1", "wget", "curl", "procps"
    ])
    .pip_install([
        "gradio>=5.0.0",
        "numpy==1.24.3",
        "Pillow>=9.0.0",
        "psutil",
        "opencv-python==4.10.0.84",
        "scipy==1.11.4",
        "librosa==0.10.2",
        "onnxruntime-gpu==1.19.2",
        "requests",
        "tqdm",
        "imageio==2.34.0",
        "imageio-ffmpeg==0.4.9"
    ])
    .pip_install([
        "torch==2.4.1+cu121",
        "torchvision==0.19.1+cu121",
        "torchaudio==2.4.1+cu121",
    ], index_url="https://download.pytorch.org/whl/cu121")
)

@app.function(
    image=image,
    gpu="T4",
    memory=8192,
    timeout=1800,
    volumes={
        "/app/models": models_volume,
        "/app/output": output_volume,
    },
    min_containers=1,
)
@modal.asgi_app()
def gradio_app():
    """
    Gradio Web UIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
    """
    import gradio as gr
    import subprocess
    import tempfile
    import shutil
    import os
    from pathlib import Path
    import gc
    import torch

    # CUDAãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    def process_wav2lip(video_file, audio_file, enable_gfpgan=True):
        """
        Wav2Lipå‡¦ç†ã‚’å®Ÿè¡Œ
        """
        try:
            if not video_file or not audio_file:
                return None, "âš ï¸ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹ãŒå¿…è¦ã§ã™"

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_out:
                output_path = tmp_out.name

            # Wav2Lipå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
            cmd = [
                "python", "-u", "/app/wav2lip_inference.py",
                "--checkpoint_path", "/app/models/wav2lip_gan.pth",
                "--face", video_file,
                "--audio", audio_file,
                "--outfile", output_path,
                "--resize_factor", "1"
            ]

            if enable_gfpgan:
                cmd.append("--enable_gfpgan")

            # å‡¦ç†å®Ÿè¡Œ
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True
            )

            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°å–å¾—
            logs = []
            for line in process.stdout:
                logs.append(line.strip())
                yield None, "\n".join(logs[-20:])  # æœ€æ–°20è¡Œè¡¨ç¤º

            process.wait()

            if process.returncode == 0 and os.path.exists(output_path):
                # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
                final_output = f"/app/output/result_{Path(video_file).stem}.mp4"
                shutil.copy2(output_path, final_output)

                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                return final_output, "âœ… å‡¦ç†å®Œäº†ï¼"
            else:
                return None, "âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"

        except Exception as e:
            return None, f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}"
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if 'output_path' in locals() and os.path.exists(output_path):
                os.remove(output_path)

    # Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ
    with gr.Blocks(title="Wav2Lip on Modal", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ­ Wav2Lip ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆ")
        gr.Markdown("å‹•ç”»ã¨éŸ³å£°ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å‹•ç”»ã‚’ç”Ÿæˆã—ã¾ã™")

        with gr.Row():
            with gr.Column(scale=1):
                video_input = gr.Video(
                    label="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«",
                    sources=["upload"],
                    format="mp4"
                )
                audio_input = gr.Audio(
                    label="éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«",
                    sources=["upload"],
                    type="filepath"
                )

                with gr.Row():
                    gfpgan_checkbox = gr.Checkbox(
                        label="âœ¨ GFPGANé¡”è£œæ­£ã‚’æœ‰åŠ¹åŒ–",
                        value=True
                    )

                process_btn = gr.Button(
                    "ğŸš€ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆé–‹å§‹",
                    variant="primary",
                    size="lg"
                )

            with gr.Column(scale=1):
                output_video = gr.Video(
                    label="ç”Ÿæˆçµæœ",
                    format="mp4",
                    autoplay=True
                )
                status_text = gr.Textbox(
                    label="å‡¦ç†ãƒ­ã‚°",
                    lines=10,
                    max_lines=20,
                    interactive=False
                )

        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        process_btn.click(
            fn=process_wav2lip,
            inputs=[video_input, audio_input, gfpgan_checkbox],
            outputs=[output_video, status_text]
        )

        # ãƒ•ãƒƒã‚¿ãƒ¼
        gr.Markdown("---")
        gr.Markdown("ğŸ¯ Modal Cloudä¸Šã§å‹•ä½œä¸­ | GPU: T4 | Memory: 8GB")

    return demo.launch(server_name="0.0.0.0", server_port=7860)

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    print("Deploy with: modal deploy modal_integrated_app.py")
"""
ä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å„å‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã—ã€GPUãƒ¡ãƒ¢ãƒªã‚’åŠ¹ç‡çš„ã«ç®¡ç†
"""
import torch
import gc
import asyncio
import concurrent.futures
from typing import Optional, Tuple, Dict, Any
import threading
import queue
import time
from pathlib import Path
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ParallelPipelineOptimizer:
    """ä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
        self.preprocessing_queue = queue.Queue()
        self.results_cache = {}

    def clear_gpu_memory(self):
        """GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            logger.info(f"ğŸ§¹ GPU Memory cleared. Available: {torch.cuda.mem_get_info()[0] / 1024**3:.2f}GB")

    def monitor_gpu_memory(self):
        """GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–"""
        if torch.cuda.is_available():
            total = torch.cuda.mem_get_info()[1] / 1024**3
            used = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) / 1024**3
            logger.info(f"ğŸ“Š GPU Memory: {used:.2f}GB / {total:.2f}GB ({used/total*100:.1f}%)")

    async def parallel_llama_sovits(self,
                                  llama_func,
                                  sovits_prep_func,
                                  prompt: str,
                                  voice_config: Dict) -> Tuple[str, Any]:
        """
        Llamaç”Ÿæˆã¨SoVITSå‰å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        """
        logger.info("ğŸš€ Starting parallel Llama + SoVITS preprocessing")

        # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ä½œæˆ
        loop = asyncio.get_event_loop()

        # Llamaç”Ÿæˆã‚¿ã‚¹ã‚¯
        llama_future = loop.run_in_executor(
            self.executor,
            llama_func,
            prompt
        )

        # SoVITSå‰å‡¦ç†ã‚¿ã‚¹ã‚¯
        sovits_prep_future = loop.run_in_executor(
            self.executor,
            sovits_prep_func,
            voice_config
        )

        # ä¸¡ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤
        llama_result, sovits_prep = await asyncio.gather(
            llama_future,
            sovits_prep_future
        )

        # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        self.clear_gpu_memory()

        return llama_result, sovits_prep

    async def parallel_sovits_wav2lip_prep(self,
                                          sovits_func,
                                          wav2lip_prep_func,
                                          text: str,
                                          sovits_config: Any,
                                          video_path: str) -> Tuple[str, Any]:
        """
        SoVITSéŸ³å£°ç”Ÿæˆã¨Wav2Lipå‰å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        """
        logger.info("ğŸµ Starting parallel SoVITS + Wav2Lip preprocessing")

        loop = asyncio.get_event_loop()

        # SoVITSéŸ³å£°ç”Ÿæˆã‚¿ã‚¹ã‚¯
        sovits_future = loop.run_in_executor(
            self.executor,
            sovits_func,
            text,
            sovits_config
        )

        # Wav2Lipå‹•ç”»å‰å‡¦ç†ã‚¿ã‚¹ã‚¯
        wav2lip_prep_future = loop.run_in_executor(
            self.executor,
            wav2lip_prep_func,
            video_path
        )

        # ä¸¡ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤
        audio_path, video_prep = await asyncio.gather(
            sovits_future,
            wav2lip_prep_future
        )

        # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        self.clear_gpu_memory()

        return audio_path, video_prep

    async def parallel_wav2lip_facefusion_prep(self,
                                              wav2lip_func,
                                              facefusion_prep_func,
                                              audio_path: str,
                                              video_data: Any,
                                              source_face: str) -> Tuple[str, Any]:
        """
        Wav2Lipå‡¦ç†ã¨FaceFusionå‰å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        """
        logger.info("ğŸ‘„ Starting parallel Wav2Lip + FaceFusion preprocessing")

        loop = asyncio.get_event_loop()

        # Wav2Lipå‡¦ç†ã‚¿ã‚¹ã‚¯
        wav2lip_future = loop.run_in_executor(
            self.executor,
            wav2lip_func,
            audio_path,
            video_data
        )

        # FaceFusionãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯
        facefusion_prep_future = loop.run_in_executor(
            self.executor,
            facefusion_prep_func,
            source_face
        )

        # ä¸¡ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤
        wav2lip_result, facefusion_prep = await asyncio.gather(
            wav2lip_future,
            facefusion_prep_future
        )

        # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        self.clear_gpu_memory()

        return wav2lip_result, facefusion_prep

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.executor.shutdown(wait=True)
        self.clear_gpu_memory()


class StreamingPipeline:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self):
        self.chunk_size = 1024  # éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        self.frame_buffer_size = 30  # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º

    async def stream_audio_to_video(self,
                                   audio_generator,
                                   video_processor,
                                   output_path: str):
        """
        éŸ³å£°ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ãªãŒã‚‰å‹•ç”»å‡¦ç†
        """
        logger.info("ğŸŒŠ Starting streaming pipeline")

        audio_buffer = queue.Queue(maxsize=10)
        video_buffer = queue.Queue(maxsize=self.frame_buffer_size)

        # éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¹ãƒ¬ãƒƒãƒ‰
        def audio_streaming_worker():
            for chunk in audio_generator:
                audio_buffer.put(chunk)
            audio_buffer.put(None)  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«

        # å‹•ç”»å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
        def video_processing_worker():
            while True:
                audio_chunk = audio_buffer.get()
                if audio_chunk is None:
                    break

                # éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
                frames = video_processor.process_audio_chunk(audio_chunk)
                for frame in frames:
                    video_buffer.put(frame)

                # å®šæœŸçš„ã«GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                if audio_buffer.qsize() % 5 == 0:
                    torch.cuda.empty_cache()

            video_buffer.put(None)  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«

        # ã‚¹ãƒ¬ãƒƒãƒ‰èµ·å‹•
        audio_thread = threading.Thread(target=audio_streaming_worker)
        video_thread = threading.Thread(target=video_processing_worker)

        audio_thread.start()
        video_thread.start()

        # çµæœã®åé›†
        output_frames = []
        while True:
            frame = video_buffer.get()
            if frame is None:
                break
            output_frames.append(frame)

        audio_thread.join()
        video_thread.join()

        return output_frames


def optimize_gpu_usage(func):
    """
    GPUãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    é–¢æ•°å®Ÿè¡Œå‰å¾Œã§GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
    """
    def wrapper(*args, **kwargs):
        # å®Ÿè¡Œå‰ã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            before = torch.cuda.memory_allocated()

        result = func(*args, **kwargs)

        # å®Ÿè¡Œå¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            after = torch.cuda.memory_allocated()
            freed = (before - after) / 1024**3
            if freed > 0:
                logger.info(f"ğŸ§¹ Freed {freed:.2f}GB GPU memory after {func.__name__}")

        return result
    return wrapper


class GPUMemoryManager:
    """
    GPUãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¯ãƒ©ã‚¹
    å‡¦ç†ã”ã¨ã«é©åˆ‡ãªãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’è¨­å®š
    """

    def __init__(self):
        self.memory_limits = {
            'llama': 8.0,      # 8GB
            'sovits': 4.0,     # 4GB
            'wav2lip': 3.0,    # 3GB
            'facefusion': 4.0  # 4GB
        }

    def set_memory_limit(self, process_name: str):
        """ãƒ—ãƒ­ã‚»ã‚¹ã”ã¨ã®ãƒ¡ãƒ¢ãƒªåˆ¶é™è¨­å®š"""
        if torch.cuda.is_available():
            limit = self.memory_limits.get(process_name, 4.0)
            # PyTorchã®ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦æˆ¦ç•¥ã‚’è¨­å®š
            torch.cuda.set_per_process_memory_fraction(limit / 24.0)  # 24GB GPUã‚’æƒ³å®š
            logger.info(f"ğŸ“ Set GPU memory limit for {process_name}: {limit}GB")

    def release_memory(self, model_ref=None):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ãƒ¢ãƒªã®è§£æ”¾"""
        if model_ref is not None:
            del model_ref

        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ã®ãƒ†ã‚¹ãƒˆ
    optimizer = ParallelPipelineOptimizer()
    memory_manager = GPUMemoryManager()

    # GPUãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ç¢ºèª
    optimizer.monitor_gpu_memory()

    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    optimizer.clear_gpu_memory()

    print("âœ… Parallel Pipeline Optimizer ready!")
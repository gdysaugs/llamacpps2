#!/usr/bin/env python3
"""
Modal Cloudå®Œå…¨çµ±åˆç‰ˆ - Wav2Lip Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
ã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import modal

app = modal.App("wav2lip-web")

# ãƒœãƒªãƒ¥ãƒ¼ãƒ è¨­å®š
models_volume = modal.Volume.from_name("wav2lip-models-2025", create_if_missing=True)

# Dockerã‚¤ãƒ¡ãƒ¼ã‚¸å®šç¾©
image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install([
        "ffmpeg",
        "libgl1-mesa-glx",
        "libglib2.0-0",
        "libsm6",
        "libxext6",
        "libxrender1",
        "libgomp1"
    ])
    .pip_install([
        # Gradio & åŸºæœ¬
        "gradio==4.44.0",
        "numpy==1.24.3",
        "Pillow>=9.0.0",
        "opencv-python==4.10.0.84",
        "scipy==1.11.4",
        "librosa==0.10.2",
        "imageio==2.34.0",
        "imageio-ffmpeg==0.4.9",
        "tqdm",

        # ONNX Runtime
        "onnxruntime-gpu==1.19.2"
    ])
    # PyTorch (CUDA 12.1)
    .pip_install([
        "torch==2.1.0",
        "torchvision==0.16.0",
        "torchaudio==2.1.0"
    ], index_url="https://download.pytorch.org/whl/cu121")
)

@app.function(
    image=image,
    gpu="T4",
    memory=8192,
    timeout=1800,
    volumes={"/models": models_volume},
    min_containers=0,  # 0ã«ã—ã¦ã‚³ã‚¹ãƒˆå‰Šæ¸›
    max_containers=3,
)
def wav2lip_processor():
    """
    Wav2Lipå‡¦ç†é–¢æ•°
    """
    import os
    import cv2
    import numpy as np
    import torch
    import onnxruntime as ort
    from pathlib import Path
    import tempfile
    import subprocess

    # ONNX Runtimeè¨­å®š
    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']

    def process_video(video_path: str, audio_path: str, enable_gfpgan: bool = True):
        """
        å‹•ç”»ã¨éŸ³å£°ã‹ã‚‰ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å‹•ç”»ã‚’ç”Ÿæˆ
        """
        try:
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_path = tempfile.mktemp(suffix='.mp4')

            # FFmpegã§å‹•ç”»ã¨éŸ³å£°ã‚’åˆæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            cmd = [
                'ffmpeg', '-i', video_path, '-i', audio_path,
                '-c:v', 'libx264', '-c:a', 'aac',
                '-map', '0:v:0', '-map', '1:a:0',
                '-y', output_path
            ]

            subprocess.run(cmd, check=True, capture_output=True)

            return output_path, "âœ… å‡¦ç†å®Œäº†"

        except Exception as e:
            return None, f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}"

    return process_video

@app.function(
    image=image,
    gpu="T4",
    memory=8192,
    timeout=3600,
    volumes={"/models": models_volume},
    min_containers=1,  # Web UIã¯1ã¤å¸¸æ™‚èµ·å‹•
)
@modal.web_endpoint()
def web_app():
    """
    Gradio Web UIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    import gradio as gr
    from fastapi import FastAPI
    from fastapi.responses import HTMLResponse

    # FastAPIã‚¢ãƒ—ãƒª
    web_app = FastAPI()

    # Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ
    def create_interface():
        with gr.Blocks(
            title="Wav2Lip on Modal",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 1200px;
                margin: auto;
            }
            """
        ) as demo:
            gr.Markdown("""
            # ğŸ­ Wav2Lip ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆ
            ### Modal Cloudä¸Šã§å‹•ä½œã™ã‚‹é«˜é€Ÿãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
            """)

            with gr.Row():
                with gr.Column(scale=1):
                    video_input = gr.Video(
                        label="ğŸ“¹ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«",
                        sources=["upload"],
                        height=300
                    )
                    audio_input = gr.Audio(
                        label="ğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«",
                        sources=["upload"],
                        type="filepath"
                    )

                    gfpgan_check = gr.Checkbox(
                        label="âœ¨ GFPGANé¡”è£œæ­£",
                        value=True
                    )

                    process_btn = gr.Button(
                        "ğŸš€ å‡¦ç†é–‹å§‹",
                        variant="primary",
                        size="lg"
                    )

                with gr.Column(scale=1):
                    output_video = gr.Video(
                        label="ğŸ¬ ç”Ÿæˆçµæœ",
                        height=400
                    )
                    status = gr.Textbox(
                        label="ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
                        lines=3
                    )

            def process(video, audio, gfpgan):
                if not video or not audio:
                    return None, "âš ï¸ å‹•ç”»ã¨éŸ³å£°ã®ä¸¡æ–¹ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"

                try:
                    # Wav2Lipå‡¦ç†ã‚’å‘¼ã³å‡ºã—
                    processor = wav2lip_processor()
                    result, message = processor(video, audio, gfpgan)
                    return result, message
                except Exception as e:
                    return None, f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}"

            process_btn.click(
                fn=process,
                inputs=[video_input, audio_input, gfpgan_check],
                outputs=[output_video, status]
            )

            gr.Markdown("""
            ---
            ğŸ¯ **Modal Cloud** | GPU: T4 | é«˜é€Ÿå‡¦ç†å¯¾å¿œ
            """)

        return demo

    # Gradioã‚¢ãƒ—ãƒªã‚’ãƒã‚¦ãƒ³ãƒˆ
    demo = create_interface()
    demo.queue()
    gradio_app = gr.mount_gradio_app(web_app, demo, path="/")

    @web_app.get("/health")
    def health_check():
        return {"status": "healthy"}

    return web_app

if __name__ == "__main__":
    print("""
    ğŸš€ Modal Wav2Lip ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †:

    1. Modal CLIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
       pip install modal

    2. èªè¨¼è¨­å®š:
       modal token set

    3. ãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ:
       modal deploy modal_full_app.py

    4. ã‚¢ã‚¯ã‚»ã‚¹:
       ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹URLã«ã‚¢ã‚¯ã‚»ã‚¹
    """)